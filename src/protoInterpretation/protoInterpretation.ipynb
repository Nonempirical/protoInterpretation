{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZCEl4S3hksBa",
    "outputId": "fe5c5ec4-074d-4b92-fff3-2e17109dc63a"
   },
   "outputs": [],
   "source": [
    "import torch, platform, sys\n",
    "\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_u-N3gUofko",
    "outputId": "00264ac1-77ff-4d56-aac5-62c2eb6a4dff"
   },
   "outputs": [],
   "source": [
    "!pip install -U \"transformers>=4.37.0\" accelerate einops tiktoken \\\n",
    "    umap-learn scikit-learn matplotlib\n",
    "\n",
    "# If you want a fresh torch, uncomment (Colab usually has a good version already)\n",
    "# !pip install -U torch --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w83s-RIGogEn",
    "outputId": "9cf5d5be-4a75-4507-db6c-38df3dad0a2a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clone (skip if you already have it)\n",
    "if not os.path.exists(\"protoInterpretation\"):\n",
    "    !git clone https://github.com/Nonempirical/protoInterpretation protoInterpretation\n",
    "\n",
    "%cd protoInterpretation\n",
    "\n",
    "# Editable install\n",
    "!pip install -e .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "buP_vS7KpSQG"
   },
   "outputs": [],
   "source": [
    "from src.protoInterpretation import (\n",
    "    ModelWrapper,\n",
    "    SamplingConfig,\n",
    "    sample_chain,\n",
    "    compute_horizon_metrics,\n",
    "    project_step_embeddings,\n",
    "    plot_entropy_curve,\n",
    "    plot_horizon_width,\n",
    "    plot_step_scatter_2d,\n",
    "    save_batch_npz,\n",
    "    save_metrics_json,\n",
    ")\n",
    "from src.protoInterpretation.model import HFModelAdapter, HFModelConfig\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnVORlsmR1fy"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "HF_TOKEN = userdata.get(\"hfKey\")  # your Colab secret name\n",
    "login(token=HF_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529,
     "referenced_widgets": [
      "01b79c59eef742559ed717747331567b",
      "357a199d6bd543eba980d32fac8976e2",
      "d94a6710909e450b9b0cc6f92e3b8f0a",
      "6e3f96e70f484ceab12227d5ef23ef3a",
      "5449d455f3694593be084145c1bb86e8",
      "33fa5d5495be437f9051ee8f570258d9",
      "9ab353885585417ea8c829adea1787ae",
      "2f45ef963ef54858bcc043b2b8ae572b",
      "36e4ac6fe172494b878a7c964f5bdbdd",
      "20c04e20350f47de85c280dc4c83a3ab",
      "95800f0f61e742338d4dae2be3023067",
      "5cf053204e4f4ccfae51e3b845e07e94",
      "ad4ed75785fe497084a4f817ebde5c17",
      "693d645456bd476e8ff2968d540f44ce",
      "c37557caa5824de8b4dce9453dda56df",
      "2080e3413b5c4014ad4ddf6171b3048a",
      "8719b0a06a874066ad4fe142ca116908",
      "8bccfe68fff6413e81ac3068f37ddd1d",
      "749f5bc8468e49c9bb6a313b4ba19b30",
      "e9d3bc54745b4298849e0c019a4b4c45",
      "1576db44d6a24890abbd8b896dc06b6a",
      "73cca7bb935e476984ec520e138f2125",
      "8cc5fedc4242450ea494988d8931b5ac",
      "6f9d5cc5aed6452686713761ec134e67",
      "d6d7ff0726e94487baaea1ac34e0065d",
      "9561cdd0e7584d2bbd9e5eb219270ee6",
      "725a1144b85c4b3ea7e65e857df43c55",
      "b7235a8a570540baa50e57757e3488a3",
      "5d324e0d626b486b8f3298595906a743",
      "17969fa4ac6d46d891a72b9091cf85f7",
      "48c483e25a2a492791e1813835a328fb",
      "1650749377ef4ae48ba3c002c98eaac8",
      "d962d9d40c1445c5b9f80e67e2c63b80",
      "b1c40c9764604392882fa38f6392bb46",
      "e4e21f16c65c42f595a949da7e93cff9",
      "cf49b42899024c7f8298b263ec223021",
      "289a1f560e1d47f6b31bbbf61186ae05",
      "24d645fa7a7a40f19a70f759cb947f48",
      "7feea694f76a407f811efd248444e429",
      "70d5d69e9d7e48338a1dec301da85737",
      "178da22d464e423caa6c0fd318a183b8",
      "37faf50d8e5c48d3adf195ae5a056821",
      "756c6cd71efe4c45911092f2f227d8ec",
      "69e6dcc308f74e78bf7b66dd779611dc",
      "5605d20d2e9c4d75bb666089341663da",
      "47e106aba0d64997855f5a61468dc5a9",
      "e51e6cf70d904388a4a74478da6ac2f9",
      "501a0e8076e043eba9e71546824636c7",
      "b8060a06133c4c26a5a6d0909a263a16",
      "eb90b1b59beb4dd99678861e6f8d4222",
      "2312f98a96634b3189434ff68918e888",
      "75ce490b17ed4dc483fa5ae22f5ca80a",
      "02f42e50541c476eae912922f4cee476",
      "aa177acd9218410f8bf8df0401950e80",
      "76e6103ab62e4a4ba3bb4dc10cf35e54",
      "6855611384f44d3bb255e9416525a5a7",
      "dcf5f51430f442778d64b29874a0ef86",
      "2ca9d04aaf33436295f5d159b8116147",
      "996bb37df0da443fa798ba8076522e29",
      "e006539b67f04c379d17c71861acbc38",
      "03f1dc150e0149f79443c3d857534114",
      "721b3dbe1be042c28afedaa584f1cfd7",
      "d927333a8e97407a9d5024f43c55f74d",
      "fb0a704056fe4996ba300fed0843af22",
      "ded3b12d733a4d7a9236b7aece52b546",
      "4f461df94d0b45348ca98eb615306831",
      "17763ad54a9b47c5bdb798628f476fe7",
      "aa87463d277f457b825b161e93cc0067",
      "4bbd5fd8404f4b25848d898e664e5139",
      "a112f6ac3a38425c97c390cd6c90f410",
      "280fbed93ce441708d0a4218f8da3559",
      "0260876c326f4bc197c7b4b3a5386ea5",
      "6e73002f897f469397e5baf58a1be491",
      "59d27a22012046d0a00bf6e43f1aa2f4",
      "4b8f99aa28c141e58a0ec0b622218fa1",
      "b403838fea6f457ba61354a442d1b4e7",
      "d1e03865f7974305819e490eb5665481",
      "acadd08f50174c118cef84ecca3e3f5f",
      "c246128349f7471ebaa45fb12a9ddcdb",
      "57d7ca7e94e640a7a53452e598f7a09d",
      "1afb3b7c09f54aaa941de2635b4c20db",
      "69e5cf29f105408ebd116223bcb9bbbe",
      "d95796e4809d4fcc96d0dcdd94d2e667",
      "0111d05b26204cc185f28ae42ed8557f",
      "02e30c3c394b4e5cb832fe4f1d6b708f",
      "a78769de17744a708955b0395042846d",
      "317349e09dea487daeafbc4b838b9914",
      "dfd7b70170e344b8babd700f209cc7f0",
      "23ba87ad4e4d457789126a8dbde77c55",
      "28b79a8b010b4a1e944c9579b5b2bed1",
      "ac6405b8a1a5443388f944060967720e",
      "14bfd130d7c840598e8fdde8505381ba",
      "da1017ae2d2944b5afe1ac6d76bf20c2",
      "21767daf93ad4337ab0418ee3e94d372",
      "f268533072e24e35a5be357a1b6444f0",
      "a8f921d4824a44e382e8424eb334226b",
      "ef6994bd8d6148ccbadd901b896b7aef",
      "770cf73eadb84ca3b9b0a709a2cc4201",
      "5e6b740f812c40c88cc79d129e61cbfc",
      "e011618d28ad49c38cdff4a8f72907ac",
      "c18fd2c75960410db912318a9c269151",
      "8d5cb23684464fb18b45c7ea7f509355",
      "6093bc1a3c544793bb7de5d702c1445a",
      "631043bbf55244dc9d4c4df012c6399f",
      "c5df9b985b994fceb074d9ef50eaeab4",
      "8fa0aff5b16543bea8c8e211e9530c3e",
      "c4f7a4b2dca54c3aa1865b5781075199",
      "9fa6387a83044a49a5323be9dd9d0969",
      "ceaf5232f910460f9b5f48960afffdcd",
      "8b31d6b5860f43e1aa14acf3f2972051",
      "ffe95e4a6c3f41a2ad4d42f74f3c979a",
      "845b02cf7f3743669c9f59f89dadced1",
      "f7663cf23b0d41609322ca417ea42df1",
      "4fae660c161645d487fbf9f0f850377b",
      "3e15022816824cf48564aa6037528acf",
      "186d930254e04c1fbdff22db634e3bc6",
      "fad013da152740f8a25a58dd08d8883f",
      "035a1954a0e74cddbfb36b6d7bd381ff",
      "542a6aa04a9d4993a6a97a185b771835",
      "80121d216408463a9ad027f4fbc299d2",
      "d058668a06834729b71826e568deb840",
      "cb2027798e494e10b14ad36466a7b537",
      "4cdc9608488d40d9aa88f6d0049dd2d7",
      "f01d1b010ac74e0aa9a9abdbd94586a3",
      "ea3a3fa8a5e34d6a9569c0e2a94da8e6",
      "0378e4468a8e4cdaac10541ff287f598",
      "ec9e17a4da894e3399b540f25b26ad52",
      "da5c31cabd9d417580d33b20c696b6a7",
      "784dd8e235a74eb88b9f16760e942ba0",
      "c0644c5d95604c60a898bf6e5f4f93dd",
      "4e022fd9c82e4122979c353f8cbaf5e0",
      "5e891b8d06c04b67893f52932ab73333"
     ]
    },
    "id": "cQKxO_qm2WAZ",
    "outputId": "ed81bbe9-df73-44a8-e8e5-9b6befcd8dc6"
   },
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Note: we do not request attention weights during sampling, so we keep the default\n",
    "# attention implementation (typically faster than forcing eager attention).\n",
    "\n",
    "model = HFModelAdapter.__new__(HFModelAdapter)\n",
    "model.config = HFModelConfig(\n",
    "    model_name_or_path=model_name,\n",
    "    dtype=\"bfloat16\"\n",
    ")\n",
    "model.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.tokenizer = tokenizer\n",
    "model.pad_token_id = tokenizer.pad_token_id\n",
    "model.model = hf_model\n",
    "model.tokenizer.padding_side = \"left\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04iyZJsJpdfG"
   },
   "outputs": [],
   "source": [
    "from src.protoInterpretation import SamplingConfig  # adjust import if yours differs\n",
    "\n",
    "prompts = [\n",
    "    # --- OPEN ---\n",
    "    \"A bat is\",\n",
    "    \"The woman in the blue dress\",\n",
    "    \"I saw\",\n",
    "    \"Something happens when there exists\",\n",
    "    \"The man\",\n",
    "    \"The man in the street\",\n",
    "\n",
    "    # --- CLOSED ---\n",
    "    \"A pencil is\",\n",
    "    \"Napoleon is\",\n",
    "    \"Photosynthesis is the process where\",\n",
    "    \"The declaration of Independence, formally\",\n",
    "    \"Photosynthesis is\",\n",
    "    \"Erosion is\",\n",
    "]\n",
    "\n",
    "cfg = SamplingConfig(\n",
    "    num_chains=256,\n",
    "    max_steps=32,\n",
    "    temperature=0.9,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    seed=42,\n",
    "    store_topk_logits=50,\n",
    "    store_attention_weights=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "EDK3VjARP1ZX",
    "outputId": "c101590c-08e8-4d24-8a0d-b6b26c8d45d8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "USE_GOOGLE_DRIVE = True  # toggle\n",
    "\n",
    "if USE_GOOGLE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    BASE_RUN_DIR = \"/content/drive/MyDrive/protoInterpretation-runs\"\n",
    "else:\n",
    "    BASE_RUN_DIR = \"./runs\"\n",
    "\n",
    "from src.protoInterpretation import (\n",
    "    sample_chain,\n",
    "    compute_horizon_metrics,\n",
    "    save_batch_npz,\n",
    "    save_metrics_json,\n",
    ")\n",
    "\n",
    "def slugify_prompt(text: str, max_len: int = 60) -> str:\n",
    "    \"\"\"\n",
    "    \"The bat is in :\" -> \"the_bat_is_in\"\n",
    "    Safe for filenames. Truncates to max_len.\n",
    "    \"\"\"\n",
    "    s = text.strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)     # non-alnum -> _\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\") # collapse + trim\n",
    "    return (s[:max_len].rstrip(\"_\")) or \"empty_prompt\"\n",
    "\n",
    "def save_horizon_run_from_prompt(prompt_text: str, batch, metrics):\n",
    "    os.makedirs(BASE_RUN_DIR, exist_ok=True)\n",
    "\n",
    "    slug = slugify_prompt(prompt_text)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_id = f\"{slug}_{timestamp}\"   # e.g. the_bat_is_in_20251218_121530\n",
    "    run_dir = os.path.join(BASE_RUN_DIR, run_id)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    save_batch_npz(batch, os.path.join(run_dir, \"batch.npz\"))\n",
    "    save_metrics_json(metrics, os.path.join(run_dir, \"metrics.json\"), batch_meta=batch.meta)\n",
    "\n",
    "    print(f\"Saved â†’ {run_dir}\")\n",
    "\n",
    "def run_prompts_and_save(model, prompts, cfg):\n",
    "    for p in prompts:\n",
    "        batch = sample_chain(model, p, cfg)\n",
    "        metrics = compute_horizon_metrics(batch)\n",
    "        save_horizon_run_from_prompt(p, batch, metrics)\n",
    "\n",
    "# Run everything\n",
    "run_prompts_and_save(model, prompts, cfg)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "u-zag_NRP5hs",
    "dz214bZT6JaJ",
    "gKxQcf0ksij-"
   ],
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
